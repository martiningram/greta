# R6 inference class objects

# base node class
inference <- R6Class(
  'inference',
  public = list(

    model = NULL,

    # size and current value of the free state
    n_free = 1L,
    free_state = 0,

    # and of the traced values
    n_traced = 1L,

    parameters = list(),
    tuning_periods = list(),

    minibatch_function = NULL,

    # free state values for the last burst
    last_burst_free_states = matrix(0),
    # all recorded free state values
    traced_free_state = matrix(0),
    # all recorded greta array values
    traced_values = matrix(0),

    # where this is in the run
    initialize = function (initial_values, model, parameters = list()) {

      self$parameters <- parameters
      self$model <- model
      self$n_free <- length(model$dag$example_parameters())
      self$free_state <- self$initial_values(initial_values)
      self$n_traced <- length(model$dag$trace_values(self$free_state))

    },

    get_feed_dict = function() {

      if (is.null(minibatch_function)) {
        # For now, this just means we return an empty feed dict
        return(list())
      else {
        return(minibatch_function())
      }

    },

    initial_values = function (user_specified) {

      # check user-provided initial values
      if (!is.null(user_specified)) {

        # check their length
        if (length(user_specified) != self$n_free) {
          stop ("each set of initial values must be a vector of length ",
                self$n_free,
                call. = FALSE)
        }

        # check they can be be used
        valid <- self$valid_parameters(user_specified)
        if (!valid) {
          stop ("The log density and gradients could not be evaluated ",
                "at these initial values.",
                call. = FALSE)
        }

        initial_values <- user_specified

      } else {

        # otherwise, try several times to generate some
        valid <- FALSE
        attempts <- 1
        while (!valid & attempts < 20) {

          initial_values <- rnorm(self$n_free, 0, 0.5)

          # test validity of values
          valid <- self$valid_parameters(initial_values)
          attempts <- attempts + 1

        }

        if (!valid) {
          stop ("Could not find reasonable starting values after ", attempts,
                " attempts. Please specify initial values manually via the ",
                "initial_values argument to mcmc",
                call. = FALSE)
        }
      }

      # set them as the state
      self$free_state <- initial_values

    },

    # check whether the model can be evaluated at these parameters
    valid_parameters = function (parameters) {

      dag <- self$model$dag
      tfe <- dag$tf_environment

      if (!exists("joint_density_adj", envir = tfe)) {
        dag$on_graph(dag$define_joint_density())
      }

      if (!exists("gradients_adj", envir = tfe)) {
        dag$on_graph(dag$define_gradients())
      }

      dag$send_parameters(parameters)
      ld <- self$model$dag$log_density()
      grad <- self$model$dag$gradients()
      all(is.finite(c(ld, grad)))

    },

    # run a burst of sampling, and put the resulting free state values in
    # last_burst_free_states
    run_burst = function () {

      stop ("no method to run a burst in the base inference class")
      self$last_burst_free_states <- free_states

    },

    # store the free state, and/or corresponding values of the target greta
    # arrays for the latest batch of raw draws
    trace = function (values = TRUE, free_state = TRUE) {

      if (free_state) {
        # append the free state trace
        self$traced_free_state <- rbind(self$traced_free_state,
                                        self$last_burst_free_states)
      }

      if (values) {
        # calculate the observed values
        last_burst_free_states <- self$last_burst_free_states
        last_burst_values <- self$trace_burst_values(last_burst_free_states)
        self$traced_values <- rbind(self$traced_values,
                                    last_burst_values)
      }

    },

    # given a matrix of free state values, get a matrix of values of the target
    # greta arrays
    trace_burst_values = function (burst_free_states) {

      values_trace <- apply(burst_free_states, 1, self$model$dag$trace_values)

      # make sure it's a matrix in the correct orientation
      if (is.matrix(values_trace)) {
        values_trace <- t(values_trace)
      } else {
        values_trace <- matrix(values_trace)
      }

      values_trace

    },

    # is the sampler in one of the tuning periods for a given parameter
    in_periods = function (periods, i, n_samples) {

      within <- function (period, fraction)
        fraction > period[1] & fraction <= period[2]

      fraction <- i / n_samples
      in_period <- vapply(periods, within, fraction, FUN.VALUE = FALSE)
      any(in_period)
    }

  )
)

# Base optimiser class
optimiser <- R6Class(
  "optimiser",
  inherit = inference,
  public = list(
    max_iterations = 100,
    tolerance = 1e-6,
    run_step = function() {
      stop('This function has to be implemented in a subclass!')
    },
    optimise = function() {
      diff <- old_obj <- Inf
      it <- 0
      while (it < self$max_iterations & diff > self$tolerance) {
        it <- it + 1
        obj <- self$run_step()
        diff <- abs(old_obj - obj)
        old_obj <- obj
      }

      # Fetch the final values
      # TODO: Maybe make this a little bit neater; not ideal that we have to
      # send the free state first.
      tf_run <- self$model$dag$tf_run
      final_free_state <- tf_run(sess$run(free_state))
      self$model$dag$send_parameters(final_free_state)

      return(list('convergence' = ifelse(it < self$max_iterations, 0, 1),
                  'iterations' = it,
                  'par' = self$model$dag$trace_values(),
                  'value' = obj))
    }
  )
)

# The adagrad optimiser
adagrad_optimiser <- R6Class(
  "adagrad_optimiser",
  inherit = optimiser,
  public = list(

    parameters = list(learning_rate = 0.8, 
                      initial_accumulator_value = 0.1,
                      use_locking = TRUE),

    initialize = function (initial_values, model, parameters = list()) {

      super$initialize(initial_values = initial_values,
                       model = model,
                       parameters = parameters)

      # get the tensorflow environment
      tfe <- self$model$dag$tf_environment
      on_graph <- self$model$dag$on_graph
      tf_run <- self$model$dag$tf_run

      # Initialise the optimiser
      on_graph(tfe$optimiser <- do.call(tf$train$AdagradOptimizer, 
                                        parameters))

      tf_run(train <- optimiser$minimize(-joint_density))

      # initialize the variables
      tf_run(sess$run(tf$global_variables_initializer()))
    },

    run_step = function() {
      tf_run <- self$model$dag$tf_run
      tf_run(sess$run(train))
      tf_run(sess$run(-joint_density))
    }
    )
)

#' @importFrom coda mcmc mcmc.list
sampler <- R6Class(
  "sampler",
  inherit = inference,
  public = list(

    acceptance_rate = 0,
    numerical_rejections = 0,
    chain_number = 1,
    n_chains = 1,

    # how often to tune during warmup
    tuning_interval = 5,

    run_chain = function (n_samples, thin, warmup, verbose, pb_update) {

      self$print_chain_number()

      self$traced_free_state <- matrix(NA, 0, self$n_free)
      self$traced_values <- matrix(NA, 0, self$n_traced)

      # if warmup is required, do that now
      if (warmup > 0) {

        if (verbose) {
          pb_warmup <- create_progress_bar("warmup",
                                           c(warmup, n_samples),
                                           pb_update)
          iterate_progress_bar(pb_warmup, 0, 0)
        } else {
          pb_warmup <- NULL
        }

        # split up warmup iterations into bursts of sampling
        burst_lengths <- self$burst_lengths(warmup,
                                            pb_update,
                                            warmup = TRUE)
        completed_iterations <- cumsum(burst_lengths)

        for (burst in seq_along(burst_lengths)) {

          self$run_burst(burst_lengths[burst], thin = thin)
          self$tune(completed_iterations[burst], warmup)
          self$trace(values = FALSE)

          if (verbose) {
            iterate_progress_bar(pb_warmup,
                                 it = completed_iterations[burst],
                                 rejects = self$numerical_rejections)
          }

        }

      }

      # scrub the free state trace
      self$traced_free_state <- matrix(NA, 0, self$n_free)

      # main sampling
      if (verbose) {
        pb_sampling <- create_progress_bar('sampling',
                                           c(warmup, n_samples),
                                           pb_update)
        iterate_progress_bar(pb_sampling, 0, 0)
      } else {
        pb_sampling <- NULL
      }

      # split up warmup iterations into bursts of sampling
      burst_lengths <- self$burst_lengths(n_samples, pb_update)
      completed_iterations <- cumsum(burst_lengths)

      for (burst in seq_along(burst_lengths)) {

        self$run_burst(burst_lengths[burst], thin = thin)
        self$trace()

        if (verbose) {
          iterate_progress_bar(pb_sampling,
                               it = completed_iterations[burst],
                               rejects = self$numerical_rejections)
        }

      }

      sampler

    },

    # print the chain number (if relevant)
    print_chain_number = function () {

      if (self$n_chains > 1) {
        msg <- sprintf("\nchain %i/%i\n",
                       self$chain_number,
                       self$n_chains)
        cat(msg)
      }

    },

    # split the number of samples up into bursts of running the sampler,
    # considering the progress bar update frequency and the parameter tuning
    # schedule during warmup
    burst_lengths = function (n_samples, pb_update, warmup = FALSE) {

      # when to stop for progress bar updates
      changepoints <- c(seq(0, n_samples, by = pb_update), n_samples)

      if (warmup) {

        # when to break to update tuning
        tuning_points <- seq(0, n_samples, by = self$tuning_interval)
        changepoints <- c(changepoints, tuning_points)

      }

      changepoints <- sort(unique(changepoints))
      diff(changepoints)

    },

    # by default, samplers have an empty tuning method
    tune = function (iterations_completed, total_iterations) {
      invisible(NULL)
    }

  )
)

hmc_sampler <- R6Class(
  "hmc_sampler",
  inherit = sampler,
  public = list(

    parameters = list(Lmin = 10,
                      Lmax = 20,
                      epsilon = 0.005,
                      diag_sd = 1),

    # tuning information for these variables
    accept_trace = NULL,
    sum_epsilon_trace = NULL,
    last_burst_length = 100L,

    initialize = function (initial_values, model, parameters = list()) {

      # initialize the inference method
      super$initialize(initial_values = initial_values,
                       model = model,
                       parameters = parameters)

      # define the draws tensor on the tf graph
      self$define_tf_hmc_draws(self$last_burst_length,
                               define_variables = TRUE)

    },

    define_tf_hmc_draws = function (burst_length, define_variables = FALSE) {

      dag <- self$model$dag
      tfe <- dag$tf_environment

      if (define_variables) {
        # define tensors for the parameters
        dag$tf_run(hmc_epsilon <- tf$placeholder(dtype = tf_float(),
                                                 shape = list()))
        dag$tf_run(hmc_L <- tf$placeholder(dtype = tf$int32,
                                           shape = list()))

        # and the sampler info
        dag$tf_run(hmc_thin <- tf$placeholder(dtype = tf$int32,
                                              shape = list()))
        tfe$log_prob_fun <- dag$generate_log_prob_function(adjust = TRUE)
      }

      # change the burst length
      tfe$hmc_burst_length <- as.integer(burst_length)

      # define the whole draws tensor
      dag$tf_run(
        hmc_all_draws <- tf$contrib$bayesflow$hmc$chain(
          n_iterations = hmc_burst_length,
          step_size = hmc_epsilon,
          n_leapfrog_steps = hmc_L,
          initial_x = tf$reshape(free_state, list(length(free_state))),
          target_log_prob_fn = log_prob_fun,
          event_dims = 0L)[[1]]
      )

      # define the thinned draws tensor
      dag$tf_run(
        hmc_thinned_draws <- tf$strided_slice(hmc_all_draws,
                                              c(0L, 0L),
                                              tf$constant(c(hmc_burst_length,
                                                            ncol(hmc_all_draws))),
                                              tf$stack(list(hmc_thin, 1L)))
      )
    },

    run_burst = function (n_samples, thin) {
      self$tf_run_burst(n_samples, thin)
    },

    # run a burst with tensorflow HMC
    tf_run_burst = function (n_samples, thin) {

      dag <- self$model$dag
      tfe <- dag$tf_environment

      # get parameters
      Lmin <- self$parameters$Lmin
      Lmax <- self$parameters$Lmax
      L <- sample(seq(Lmin, Lmax), 1)
      epsilon <- self$parameters$epsilon

      # set up dict
      tfe$hmc_values <- list(free_state = as.matrix(self$free_state),
                             hmc_L = L,
                             hmc_epsilon = epsilon,
                             hmc_thin = thin)
      dag$tf_run(hmc_dict <- do.call(dict, hmc_values))

      # if the required burst length has changed, redefine the draws
      if (n_samples != self$last_burst_length) {
        self$define_tf_hmc_draws(n_samples)
        self$last_burst_length <- n_samples
      }

      # run sampler
      free_state_draws <- dag$tf_run(sess$run(hmc_thinned_draws,
                                              feed_dict = hmc_dict))

      # get required variables
      self$last_burst_free_states <- free_state_draws
      n_draws <- nrow(free_state_draws)
      self$free_state <- free_state_draws[n_draws, ]

      # was each sample an acceptance or a rejection?
      accept_trace <- as.numeric(diff(free_state_draws[, 1]) != 0)
      self$accept_trace <- c(self$accept_trace, accept_trace)

    },

    # run the sampler for n_samples (possibly thinning)
    r_run_burst = function (n_samples, thin) {

      self$last_burst_free_states <- matrix(NA, n_samples, self$n_free)

      # unpack options
      epsilon <- self$parameters$epsilon
      diag_sd <- self$parameters$diag_sd
      L <- seq(self$parameters$Lmin,
               self$parameters$Lmax)

      dag <- self$model$dag
      n_free <- self$n_free
      x <- self$free_state

      # track acceptance and numerical rejections
      accept_trace <- rep(0, n_samples)

      # set initial location, log joint density and gradients
      dag$send_parameters(x)
      grad <- dag$gradients()
      logprob <- dag$log_density()

      # loop through iterations
      for (i in 1:n_samples) {

        # copy old state
        x_old <- x
        logprob_old <- logprob
        grad_old <- grad
        p <- p_old <- rnorm(n_free)

        # start leapfrog steps
        reject <- FALSE
        n_steps <- ifelse(length(L) == 1, L,
                          base::sample(L, 1))

        for (l in seq_len(n_steps)) {

          # step
          p <- p + 0.5 * epsilon * grad * diag_sd
          x <- x + epsilon * p * diag_sd

          # send parameters
          dag$send_parameters(x)
          grad <- dag$gradients()

          # check gradients are finite
          if (any(!is.finite(grad))) {
            reject <- TRUE
            break()
          }

          p <- p + 0.5 * epsilon * grad * diag_sd

        }

        # if the step was bad, reject it out of hand
        if (reject) {

          self$numerical_rejections <- self$numerical_rejections + 1
          x <- x_old
          logprob <- logprob_old
          grad <- grad_old

        } else {

          # otherwise do the Metropolis accept/reject step

          # inner products
          p_prod <- 0.5 * sum(p ^ 2)
          p_prod_old <- 0.5 * sum(p_old ^ 2)

          # acceptance ratio
          logprob <- dag$log_density()
          log_accept_ratio <- logprob - p_prod - logprob_old + p_prod_old
          log_u <- log(runif(1))

          if (log_u < log_accept_ratio) {

            accept_trace[i] <- 1

          } else {

            # on rejection, reset all the parameters and push old parameters to
            # the graph for the trace
            x <- x_old
            logprob <- logprob_old
            grad <- grad_old

          }

        }

        # store the values of the free state for this burst
        if (i %% thin == 0) {
          self$last_burst_free_states[i %/% thin, ] <- x
        }


      }

      # assign the free state and tuning information at the end of this burst
      self$free_state <- x
      self$accept_trace <- c(self$accept_trace, accept_trace)

    },

    # overall tuning method
    tune = function(iterations_completed, total_iterations) {
      self$tune_epsilon(iterations_completed, total_iterations)
      self$tune_diag_sd(iterations_completed, total_iterations)
    },

    tune_epsilon = function (iterations_completed, total_iterations) {

      # tuning periods for the tunable parameters (first 10%, last 60%)
      tuning_periods <- list(c(0, 0.1), c(0.4, 1))

      # whether we're tuning now
      tuning_now <- self$in_periods(tuning_periods,
                                    iterations_completed,
                                    total_iterations)

      if (tuning_now) {

        # epsilon & tuning parameters
        epsilon <- self$parameters$epsilon
        accept_group <- 50
        target_acceptance <- 0.651
        kappa <- 0.75
        gamma <- 0.1

        # acceptance rate over the accept_group samples
        start <- max(1, iterations_completed - accept_group)
        end <- iterations_completed
        accept_rate <- mean(self$accept_trace[start:end], na.rm = TRUE)

        # decrease the adaptation rate as we go
        adapt_rate <- min(1, gamma * iterations_completed ^ (-kappa))

        # shift epsilon in the right direction, making sure it never goes
        # negative
        neg_eps <- -(epsilon + sqrt(.Machine$double.eps))
        adaptation <- adapt_rate * (accept_rate - target_acceptance)
        new_epsilon <- epsilon + pmax(neg_eps, adaptation)

        # update it
        self$parameters$epsilon <- new_epsilon

        # keep track of the *sum of epsilon* in the second half of warmup so
        # we can average properly later, accounting for different burst
        # sizes
        progress_fraction <- iterations_completed / total_iterations
        if (progress_fraction > 0.5) {
          self$sum_epsilon_trace <- c(self$epsilon_trace,
                                      epsilon * accept_group)
        }

        # if this is the end of the warmup, get the averaged epsilon for the
        # second half of the warmup and put it back in for the parameter
        if (progress_fraction == 1) {
          n_traced <- total_iterations / 2
          final_epsilon <- self$sum_epsilon_trace / n_traced
          self$parameters$epsilon <- final_epsilon
        }

      }

    },

    tune_diag_sd = function (iterations_completed, total_iterations) {

      # when, during warmup, to tune this parameter (after epsilon, but stopping
      # before halfway through)
      tuning_periods <- list(c(0.1, 0.4))

      tuning_now <- self$in_periods(tuning_periods,
                                    iterations_completed,
                                    total_iterations)

      if (tuning_now) {

        samples <- self$traced_free_state
        dups <- duplicated(samples)
        if (length(dups) > 0) {
          samples <- samples[!dups, , drop = FALSE]
        }
        n_accepted <- nrow(samples)

        # provided there have been at least 5 acceptances in the warmup so far
        if (n_accepted > 5) {

          # get the sample posterior variance and shrink it
          sample_var <- sample_variance(samples)
          shrinkage <- 1 / (n_accepted + 5)
          var_shrunk <- n_accepted * shrinkage * sample_var + 5e-3 * shrinkage
          self$parameters$diag_sd <- sqrt(var_shrunk)

        }

      }

    }

  )
)

